import torch
import torch_geometric
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
import networkx as nx
import community as community_louvain
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Create a sample graph (social network) using NetworkX
G = nx.erdos_renyi_graph(100, 0.05)  # A random graph with 100 nodes and 5% edge probability

# Visualization of the graph
plt.figure(figsize=(8, 6))
nx.draw(G, with_labels=True, node_size=100, font_size=10)
plt.title("Social Network (Random Graph)")
plt.show()

# Convert the NetworkX graph to PyTorch Geometric format
def nx_to_torch_geometric(G):
    edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()
    x = torch.eye(G.number_of_nodes(), dtype=torch.float)  # Identity matrix as node features (simple example)
    data = torch_geometric.data.Data(x=x, edge_index=edge_index)
    return data

data = nx_to_torch_geometric(G)

# Get communities using the Louvain method
def louvain_community_detection(G):
    partition = community_louvain.best_partition(G)
    return partition

partition = louvain_community_detection(G)
num_communities = len(set(partition.values()))
print(f"Number of communities: {num_communities}")

# Define the GCN model
class GCNModel(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCNModel, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)  # Output layer matches the number of communities
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)  # Output from this layer
        return x

# Initialize the GCN model
hidden_channels = 64
model = GCNModel(in_channels=data.x.shape[1], hidden_channels=hidden_channels, out_channels=num_communities)

# Loss function and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training the model
def train(model, data, epochs=100):
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        out = model(data)

        # Get labels from Louvain partition (zero-indexed labels for CrossEntropyLoss)
        labels = torch.tensor([partition[int(node)] for node in range(len(data.x))], dtype=torch.long)
        
        # Ensure labels are zero-indexed
        labels = labels - labels.min()  # In case labels aren't zero-indexed
        
        loss = criterion(out, labels)
        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')

train(model, data)

# After training, get the learned embeddings
model.eval()
embeddings = model(data)

# Use KMeans clustering to detect communities based on learned embeddings
kmeans = KMeans(n_clusters=num_communities)
kmeans.fit(embeddings.detach().numpy())
labels = kmeans.labels_

# Visualize the communities using PCA (dimensionality reduction)
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings.detach().numpy())

plt.figure(figsize=(8, 6))
for i in range(num_communities):
    plt.scatter(reduced_embeddings[labels == i, 0], reduced_embeddings[labels == i, 1], label=f'Community {i+1}')
plt.title("Community Detection Using GCN and KMeans Clustering")
plt.legend()
plt.show()

# Print predicted communities for each node
print("Predicted Communities (KMeans labels):", labels)

