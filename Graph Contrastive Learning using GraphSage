#This code implements Graph Contrastive Learning using GraphSAGE and contrastive loss (NT-Xent/InfoNCE) to learn robust node embeddings. The key components include:  

#1. Graph Augmentation: Creates two augmented graph views via edge dropout and feature corruption to introduce variability.  
#2. Graph Encoder: Uses a two-layer GraphSAGE model to generate node embeddings.  
#3. Contrastive Learning: Encourages embeddings of the same node (across augmented views) to be similar, while pushing apart different nodes using **cosine similarity** and **contrastive loss**.  
#4. Visualizations:
#  - Graph Structure: Displays original and augmented graphs.
#   - Embedding Evolution: Shows how node embeddings change over epochs using t-SNE.
#  - Similarity Heatmap: Illustrates how contrastive learning improves embedding alignment.  
# By the end of training, nodes with similar structures and features have aligned representations, demonstrating the effectiveness of contrastive learning in graphs. 

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from sklearn.manifold import TSNE
import seaborn as sns

# Graph Encoder (GraphSAGE)
class GraphEncoder(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super(GraphEncoder, self).__init__()
        self.conv1 = SAGEConv(in_dim, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, out_dim)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return x

# Contrastive Loss (NT-Xent / InfoNCE)
class ContrastiveLoss(nn.Module):
    def __init__(self, tau=0.5):
        super(ContrastiveLoss, self).__init__()
        self.tau = tau

    def forward(self, z1, z2):
        sim_matrix = F.cosine_similarity(z1.unsqueeze(1), z2.unsqueeze(0), dim=2)
        positives = torch.exp(torch.diag(sim_matrix) / self.tau)
        negatives = torch.exp(sim_matrix / self.tau).sum(dim=1)
        loss = -torch.log(positives / negatives).mean()
        return loss, sim_matrix

# Graph Augmentation (Edge Dropout, Feature Corruption)
def augment_graph(data, drop_prob=0.2):
    edge_index = data.edge_index.clone()
    num_edges = edge_index.size(1)
    
    # Drop edges randomly
    mask = torch.rand(num_edges) > drop_prob
    edge_index = edge_index[:, mask]

    # Corrupt node features
    x = data.x.clone()
    noise = torch.randn_like(x) * 0.1
    x += (torch.rand(x.shape) < drop_prob).float() * noise  
    
    return Data(x=x, edge_index=edge_index)

# Visualize Graph
def plot_graph(edge_index, title="Graph"):
    G = nx.Graph()
    edges = edge_index.numpy().T
    G.add_edges_from(edges)
    
    plt.figure(figsize=(5, 5))
    nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500)
    plt.title(title)
    plt.show()

# Visualize Embeddings in 2D
def plot_embeddings(embeddings, title="Node Embeddings"):
    embeddings = embeddings.detach().cpu().numpy()
    
    # Use t-SNE for 2D projection
    reducer = TSNE(n_components=2, perplexity=5, random_state=42)
    embeddings_2d = reducer.fit_transform(embeddings)

    plt.figure(figsize=(5, 5))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=np.arange(len(embeddings)), cmap="viridis", s=100, edgecolors='black')
    plt.colorbar(label="Node Index")
    plt.title(title)
    plt.show()

# Visualize Contrastive Learning (Similarity Heatmap)
def plot_similarity(sim_matrix, title="Pairwise Cosine Similarity"):
    sim_matrix = sim_matrix.detach().cpu().numpy()
    plt.figure(figsize=(6, 5))
    sns.heatmap(sim_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True)
    plt.title(title)
    plt.xlabel("Nodes")
    plt.ylabel("Nodes")
    plt.show()

# Training Pipeline with Visualizations
def train_contrastive(graph_data, model, optimizer, loss_fn, epochs=100):
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        
        # Generate two augmented graphs
        data_1 = augment_graph(graph_data)
        data_2 = augment_graph(graph_data)

        # Compute embeddings
        z1 = model(data_1.x, data_1.edge_index)
        z2 = model(data_2.x, data_2.edge_index)

        # Compute contrastive loss
        loss, sim_matrix = loss_fn(z1, z2)
        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

        # Visualize contrastive learning process at epoch milestones
        if epoch in [0, epochs // 2, epochs - 1]:
            print(f"Visualizing at epoch {epoch}...")
            plot_embeddings(z1, title=f"Epoch {epoch} - View 1 Embeddings")
            plot_embeddings(z2, title=f"Epoch {epoch} - View 2 Embeddings")
            plot_similarity(sim_matrix, title=f"Epoch {epoch} - Similarity Heatmap")

    return z1  # Return final embeddings

# Generate a Toy Graph
num_nodes = 10
num_features = 5
x = torch.randn((num_nodes, num_features))  
edge_index = torch.randint(0, num_nodes, (2, 20))  

graph_data = Data(x=x, edge_index=edge_index)

# Visualize the Original Graph
plot_graph(edge_index, title="Original Graph")

# Visualize Augmented Graph Views
plot_graph(augment_graph(graph_data).edge_index, title="Augmented Graph View 1")
plot_graph(augment_graph(graph_data).edge_index, title="Augmented Graph View 2")

# Model and Optimizer
model = GraphEncoder(in_dim=num_features, hidden_dim=16, out_dim=8)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = ContrastiveLoss()

# Train and Get Embeddings
final_embeddings = train_contrastive(graph_data, model, optimizer, loss_fn, epochs=50)

# Final Embeddings Visualization
plot_embeddings(final_embeddings, title="Final Node Embeddings after Training")
